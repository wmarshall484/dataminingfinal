%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[]{units}
\usepackage{url}

\title{\LARGE \bf
Analysis of Current Data Mining Techniques on SMS Spam Filtering
%I Got Them Research Paper Writing Blues...
%Intoxicated Cells: a Prospective Study of the Effects of Tequila on Pluripotent Stem Cells
}

\author{William Marshall$^1$%
%\thanks{*This work was supported by SUNFEST}% <-this % stops a space
\thanks{$^2$Lehigh University, USA {\tt\small wcm214@lehigh.edu}}
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The necessity of a fast, accurate, easily trainable spam detection
system is reflected in the current proliferation of SMS spam
messages. By taking advantage of the suite of machine learning
algorithms integrated into WEKA, we provide an analysis of SMS spam
detection accuracy across severak different learning models, and
present the Naive Bayes algorithm as a primary choice.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Short Message Service (SMS) messages are one of the primary means of
communication. Between the years of 2007 and 2010, the number of sent
SMS messages tripled, and the global rate of message sending increased
to 200,000 texts per second [1]. Along with the increased usage of SMS
messages, the proportion of SMS spam messages has also not only risen
to match the new levels of traffic, but has accelerated to the point
where parts of Asia classified 30\% of all SMS messages as spam in
2012 [2]. Although SMS spam is not as prevelant as email spam -- which
constituted 70.7\% of all emails in Q2 2013 -- there is still a strong
motivation for cellular companies to employ effective spam detection
systems, since it's likely that customers are willing to pay more for
a service that is insulated from spam.

Many modern approaches to spam detection are founded upon the research
that stems from the identification of email spam. Email spam detection
benefits from features such as a subject header, initial greetings,
and the IP address of the sender. Such features are not applicable to
SMS messages which have an upper limit of 160 characters, providing
overall less information to perform classification. Additionally,
since SMS spam is less pervasive than email spam, its classification
is made more difficult due to the lower prior probability. Blanzieri
at al. were able to achieve an email spam misclassification rate of
only 1.17\% [3], which is a daunting task for SMS spam detection.

The differences between SMS and email spam detection warrant a
reevaluation of the effectiveness of current learning
models. Fortunately, the University of Waikato has provided WEKA: a
toolkit that conveniently provides implementations of many well known
and vetted classification, association, and clustering techniques. The
algorithms included in WEKA and tested in this paper are by no means
an exhaustive analysis of the spam detetion literature, yet this paper
should function well as an introduction for those who are new to
machine learning, WEKA, and spam detection.

\section{Dataset}
The dataset in question was collected from the UCI Machine learning
repository,which contains 5574 text messages, 425 ofwhich were spam,
the rest of which are non-spam. This dataset was chosen because its
spam to non-spam ratio of 7.6\% is approximates the 10.5\% SMS spam
rate found in nature[6]. The data was collected from a website
called Grumbletext,where users post the text of spam messages that
they have recieved, and was manually inspected and classified.


\section{Preprocessing}
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/perception-pipeline.jpg}
\caption{ The perception system is driven by registered depth and
  color images paired with an estimate of the RGB-D sensor's pose in
  the robot's coordinate frame. The dimensionality of the data
  processed by each stage is indicated in the figure, illustrating
  places where dimensionality reduction is used to improve efficiency.
}
\label{fig:perception-pipeline}
\end{figure}

\subsection{Token and Frequency Extraction}

The data was provided as raw text, where each line corresponded to a
class label and a space, followed by the text of the SMS message. The
end goal of the preprocessing stage is to transform the data into a
.arff format, where each unique word corresponds to a nominal feature
that is either a 1, if present in the text message, or a 0 if it was
not present. As such, it was necessary to take to union of each SMS
message's words across all messages in the dataset. This was
accomplished using the std::map class included in the standard
template library of c++.  The initial preprocessing, resulted in
18,152 unique words.  

Upon examination, it was discovered that many of the words contained
special characters that falsely inflated the number of unique words;
for example, in addition to having an entry for the string ``apple'',
there would also be an entry for the string: ``apple.'', ``apple!'',
and ``apple?''. Thus, it was necessary to remove all special
characters that might cause confusion within a unique feature, so any
character within the following
string \begin{verbatim}!@#\%^&*()<>.,\end{verbatim} was replaced with
a space (subequently, all spurious spaces are removed). After this
preprocessing stage, only 8,243 unique strings remained.

Many of the words that remained were ones such as ``the'' or ``a''
that were ubiquitous across both spam and non-spam messages. Rather
than have high frequency yet low information features, the strings
that were used more that 500 or less than 5 times were simply deleted
from the feature space. In addition, the top 50 most frequent words
were alse deleted, finally resulting in 2,591 unique strings. 

\subsection{Chi-squared selection}

Unfortunately, having several thousands of attributes is prohibitively
expensive for learning techniques that don't scale well with
dimensionality. Our attempts to run the J48 decision-tree learning on
insatances 2,591 attributes took several days, and eventually resulted
in an overflow of the JVM's heap space. Therefore, it was necessary to
further reduce the dimensionality to even begin to obtain data.

The chi-squared independence test provided 

As the messages were iterated over, their relative
spam and non-spam frequencies were recoreded.






\subsection{Coarse Recognition and Localization}


\subsection{Refined Recognition and Localization}


\subsection{Tracking}
The above process is applied to a small number of observed point
clusters, while the rest are left as unidentified. All clusters are
then passed to a tracking component that models the kinematics of both
identified and unidentified point clouds. Observation integration is
achieved by maintaining a track state that includes both object
position and velocity in a Kalman filtering framework.

Inspired by the linear arrangement of objects on a long table when
viewed from off-axis, observations are aligned with existing estimates
using the Needleman-Wunsch optimal matching algorithm
\cite{NeedlemanWunsch1970}. This algorithm provides a principled way
to match predictions of a linear chain of states to observations while
taking potential object identification as well as position into
account. This mechanism supports the presence of multiple objects
within a single view by matching observations to existing tracks, or
determining that an observation necessitates the initialization of a
new track.

% In the usual application of this algorithm to string
% alignment, \textit{skip} characters may be added at any point in one
% or the other strings. These are points at which two strings do not
% align, but whose removal from one string allows a likely match to
% continue beyond the missed location. The algorithm thus prevents us
% from aligning a track identified as one object with an observation
% identified as a different object type, and provides the necessary
% mechanism to inject new tracks and deal with data dropout.

The motion model used for tracking is one of zero
acceleration. Furthermore, we are interested in the problem of
tracking objects undergoing identical motion. We take advantage of
this commonality by initializing the position covariance estimate of a
new track by a sensor noise model as is usual, but carry over both
velocity value and covariance from previous tracks. This causes the
velocity estimate of successive tracks to adhere very tightly to a
robust running average.

Final reporting from the tracker is governed by a configuration
parameter indicating how many observations of any given track are
needed before reporting is warranted. When a recognized object track
is reported, an estimate of its position on the table plane is
extracted from the Kalman filter, and a bounded history of orientation
estimates provided by the localization optimization is passed through
a RANSAC \cite{Fischler1981:RANSAC} procedure to compute a robust
mean. The special consideration given to object yaw is due to the
expectation that yaw measurements will be the noisiest axis of
observation due to varying geometry occlusion as the view of an object
changes over time. Consider, for example, a round watering can with a
long spout. The yaw estimate for such an object will likely be much
better when computed from views in which the spout is visible than
those where it can not be seen.

% If the spout is not visible from some views, the yaw estimate
% is likely to be much less accurate than that estimated from views that
% capture at least part of the spout geometry.

% \item Object clustering: When looking down on objects with overhanging
%   geometry, we found that connected depth components weren't the best
%   model for the point clouds associated with each object. Histogram
%   objects along the table's long axis.

\section{Manipulation}

When an object is detected, the pick-and-place manipulation pipeline
gets called to pick up the approaching object and move it to a desired
location. The manipulation pipeline is comprised of three main
components, namely grasping, motion planning and execution. Before the
pipeline is called, an arm is chosen to pick up the object. Currently
we are simply alternating between the arms. If no feasible grasps for
the first arm are computed, we have found that quite often the other
arm is capable of performing the pick-and-place action after receiving
the next observation.

Note, that the same system is used for manipulating both static and
dynamic objects. In the case of the static object, the entire pipeline
just executes immediately, instead of waiting for the object to enter
the robot's workspace.

\subsection{Grasping}

Reliable grasping of a static object by a robot's end-effector can be
a difficult task given the uncertainty in each part of the system,
from the object pose estimation to the robot's mechanical
calibration. Additional problems arise when the object is moving at a
fast speed. To simplify grasp planning, we exploit the fact that we
have a well-defined set of objects to be manipulated. This allows us
to restrict the set of grasps for each type of object to ones that we
have previously tested and saved in a database. Once an object is
detected, the grasps are retrieved and checked for feasibility. We
will now explain each component of the grasp selection process.

\textit{\textbf{Types of Grasps.}}
The process of picking up an object requires the end-effector to move between three different 6-DOF poses:
\begin{itemize}
\item \textit{pregrasp -} An end-effector pose that is offset from the object (see Figure~\ref{fig:recording_grasps-photo}, left). The motion planner is used to plan a collision free trajectory from the arm's waiting configuration to a valid configuration with the end-effector at the pregrasp pose.
\item \textit{grasp -} The pose of the end-effector in which its fingers are in position to reliably enclose part of the object (see Figure~\ref{fig:recording_grasps-photo}, middle).
\item \textit{postgrasp -} The end-effector pose after it moved away from the surface with the object grasped. The motion planner is then used to compute a feasible motion for the arm that takes the end-effector from the postgrasp to the desired place location.
\end{itemize}

\textit{\textbf{Recording Grasps.}}  Prior to manipulating a new
object, we create a set of grasps for it. We found the grasp recording
process to be simple and reliable. The process begins by placing the
object within the workspace of either one of the arms and within the
RGB-D sensor's view. Once the item is localized, the user is asked to
move the end-effector to the desired pregrasp pose and the 6-DOF pose
of the end-effector in the coordinate frame of the object is
recorded. The process is repeated for the grasp pose, and the two
grasps are then added to the grasp database for that object type. This
pair of grasps defines a collision trajectory that brings the end
effector into contact with the target object. The angle of this
trajectory is chosen to improve the chances of a mechanically stable
grip. In general, we found that a single set of postgrasp positions
can be used for all object types so the user is not required to record
a postgrasp for each object.  Instead, the grasp orientation is paired
with a set of postgrasp translations above the object for each object
type. In our experiments, the set of grasps for each object contains
between two and twenty pregrasp-grasp pairs. See
Figure~\ref{fig:recording_grasps-photo} for an example.

\begin{figure}[t]
\centering
\includegraphics[width=0.31\columnwidth]{figures/croc-record_pregrasp.jpg}
\includegraphics[width=0.31\columnwidth]{figures/croc-record_grasp.jpg}
\includegraphics[width=0.31\columnwidth]{figures/croc-grasps-outside_view.png}
\caption{
From left to right, the user moves the end-effector to the pregrasp pose and then to the grasp pose. The pair of grasps is added to the database as they appear on the right.
}
\label{fig:recording_grasps-photo}
\end{figure}

\textit{\textbf{Grasp Selection.}}  Upon detection of a new object, we
use its estimated pose and velocity to predict $t_{pred}$ and
$x_{pred}$. $t_{pred}$ is the time at which the object will be at the
center of the workspace of the arm and $x_{pred}$ is the predicted
6-DOF object pose in the world. The object type is used to retrieve a
set of pregrasps and grasps from the database. For each pregrasp,
$p_i$, we use $x_{pred}$ to compute $x_{p_i}$, the pregrasp pose in
the world frame for when the object will be at its predicted pose. The
same is done for each grasp $g_i$ to compute $x_{g_i}$.

Before we add $\{x_{p_i}, x_{g_i}\}$ to $X_{pg}$, the set of feasible
pregrasp-grasp tuples, the following two feasibility checks are
performed on each tuple:
\begin{itemize}
% \item \textit{chase grasps -} It is very difficult to pick up a moving
%   object using a grasp motion that is \textit{chasing} it along the
%   direction of its velocity, $v_{object}$.  It rarely succeeds if
%   $v_{object}$ is equal to or greater than the maximum speed of the
%   end-effector along the grasp motion. Thus, we chose to filter out
%   the chase grasps completely by detecting if the pregrasp to grasp
%   motion is moving in the same direction as $v_{object}$. We do this
%   by computing $c = \|v_{object}\| \cdot \|x_{g_i}\|$. If $c <
%   k^\circ$, the tuple is deemed too similar to the object's velocity
%   and it is rejected. In our experiments, we set $k$ to correspond to
%   an angular difference of $-30^\circ$.
\item \textit{chase grasps -} It is very difficult to pick up a moving
  object using a grasp motion that is \textit{chasing} it along the
  direction of its velocity, $v_{object}$.  It rarely succeeds if
  $v_{object}$ is equal to or greater than the maximum speed of the
  end-effector along the grasp motion. Thus, we chose to filter out
  the chase grasps completely by detecting if the pregrasp to grasp
  motion is moving in the same direction as $v_{object}$. We do this
  by computing $c = \frac{v_{object} \cdot (x_{p_i} -
    x_{g_i})}{\|v_{object}\|\|x_{p_i} - x_{g_i}\|}$. If $c <
  k^\circ$, the tuple is deemed too similar to the object's velocity
  and it is rejected. In our experiments, we set $k$ to correspond to
  an angular difference of $-30^\circ$.

 
\item \textit{kinematics -} We check if each $x_{p_i}$ and $x_{g_i}$
  is kinematically feasible for the arm of the robot using an inverse
  kinematics solver to compute the corresponding joint
  configuration. If either is invalid, the tuple is rejected. In
  Figure~\ref{fig:kinematics_feasibility_check}, six pregrasp-grasp
  tuples are shown for the basket. After they are checked for
  kinematic feasibility, the ones in red (darker color) are discarded
  because they are unreachable by the right arm.
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/covered_basket-grasps-kinematics_check.png}
\caption{
The end-effectors in green are included in $X_{pg}$, while the ones shown in red are discarded by the kinematic feasibility check.
}
\label{fig:kinematics_feasibility_check}
\end{figure}

Given a set of valid grasps, a heuristic is typically used to select
the best grasp to plan for~\cite{WGGrasping}. However, this approach
can be overly restrictive, as the feasibility of the grasp depends on
the estimated time of arrival of the object. The better option is to
let the planner itself figure out which grasp to plan to given the
time constraint.
%a common approach to choosing which one to plan to, is to rank them using heuristics such as the number of sensed object points that fit inside the gripper or the distance from the object center~\cite{WGGrasping}. These heuristics are used because the grasp is planned on the fly and its quality is unknown. Given that our grasps were recorded previously, we are confident in their reliability and how to choose between them is unclear. 

% Thus, instead of choosing a single pregrasp-grasp tuple to target, the
% entire set of pregrasps in $X_{pg}$ is sent to the motion planner,
% allowing the planner to determine the $\{x_{p_j}, x_{g_j}\}$ in
% $X_{pg}$ that is feasible and optimizes costs well. After a plan is
% computed by the planner, the final waypoint of the planned path to the
% pregrasp, $x_{p_j}$, chosen by the planner. We use the corresponding
% grasp, $x_{g_j}$ as the grasp pose.

Thus, instead of choosing a single pregrasp-grasp tuple to target, the
entire set of pregrasps in $X_{pg}$ is sent to the motion
planner. When the planner finds a collision-free trajectory to an
element of $X_{pg}$, we select the corresponding grasp, $x_{g_j}$ as
the grasp pose. Computing the postgrasp is a straightforward process,
generic across object types. Given the grasp, $x_{g_j}$, we compute
the postgrasp, $x_{po_j}$, by searching through the set of postgrasps
for a kinematically feasible arm configuration that is as far from the
table as possible. In our experiments, we found that the $x_{po_j}$
chosen was typically within 12cm of $x_{g_j}$.

\subsection{Motion Planning}

% In an environment where robots work alongside people, it is important
% that the robot's behavior is consistent and predictable. This way, any
% human workers nearby can feel comfortable and safe around it. In
% particular, for a robot to be an effective member of a team on an
% assembly line that sorts moving objects into bins, the robot motions
% needs to be consistent, predictable and as close to optimal as
% possible in order for people to be able to work alongside and within
% the workspace of the robot. With this in mind, we implemented a
% heuristic search-based approach to generate motion plans for each of
% the robot's arms independently.

Heuristic searches such as A* provide strong theoretical guarantees
such as completeness and optimality or bounds on
suboptimality~\cite{Pearl-heur}. Their generality allows for complex
constraints and cost functions, while providing good cost minimization
and consistency in the solution. Consistency here implies that given
similar input, similar output is produced, thus making the robot's
motions more predictable.

Motion planning for pick-and-place of moving objects needs to be
performed as fast as possible so the robot has enough time to execute
the computed motions in time. To combat the high dimensionality of the
planning problem, we employ a heuristic search based approach that
uses an anytime variant of A* called ARA*~\cite{LikGorThr-ara} that
quickly finds an initial, and possibly sub-optimal, solution and
repairs it while deliberation time allows. The approach also relies on
a compact graph representation and informative heuristics to provide
real time performance. Details on this approach and applications to single and dual-arm motion planning can be found in~\cite{Cohen2, Cohen3}. 

We configured the planner to search in a 7 dimensional task space, $\{{x, y, z, roll, pitch, yaw, \theta}\}$, that represents the 6-DOF pose of the end-effector in the world frame, coupled with $\theta$, the position of the redundant degree of freedom in the robot's arm. This representation can be used when planning for a robot with one or more 7-DOF manipulators, such as the PR2 robot. In our experiments, we execute two independent instances of the planner, one for each arm. The cost function we use is aimed to minimize the 6-DOF path length of the end-effector.

During the pick action, the planner is called to plan a path from the \textit{waiting} configuration of the arm to one of the pregrasps in $X_{pg}$. Before planning begins, the geometry of the fixed objects in the robot's workspace are added to the collision representation. In our experiments, this included the conveyor belt and bins that surrounded the robot. After the robot grasps the object, the collision geometry of the object is attached to the robot's collision model. During the place action, a plan is requested from the postgrasp to any one of the drop poses above the bins. Note that there is nothing constraining the user from having the robot gently place the item on a surface instead. Now that a potentially fragile object (or an object filled with something) is grasped in the robot's end-effector, we impose an upright path constraint on the planner when computing a path for the place action. The constraint requires the planner to maintain the initial $\{roll, pitch, yaw\}$ of the end-effector throughout the path with a small tolerance in each dimension.
 
After a path is computed, we pass it through a simple deterministic shortcutting routine that can deal effectively with discretization artifacts. We found that only a single pass through the points was necessary. In our experiments, the entire shortcutting step lasted between 5-10ms, including checking the interpolated motions for collisions every $2^{\circ}$.

The ability to accurately predict the time it takes to execute a
trajectory is essential to picking up a quickly moving object. Being
that our motion planner plans solely for the kinematics of the arm, we
then perform a final post-processing step, in which the shortest
timing intervals between points are computed that enforce the robot's
dynamic constraints~\cite{IterativeSmoother}. The waypoint locations
themselves are not moved, instead feasible velocities and
accelerations are assigned. We found in our experiments that on
average, for a trajectory from an arm's initial configuration to a
pregrasp pose, the predicted execution timing and the actual
trajectory execution timing differ by approximately \unit[80]{ms}. The
pregrasp-to-grasp trajectory is much shorter, which provides for a
much smaller temporal prediction error, thus making the pregrasp pose
a useful waypoint at which to make final timing adjustments to achieve
a precise collision.

\subsection{Execution}

Given that the object is moving at a reasonable pace, the pick action
is substantially more difficult to execute than the place because very
precise timing is needed to succesfully pick up the object with a firm
grasp. For example, in our experiments the objects moved at
approximately \unitfrac[33]{cm\,}{\,s} across the robot's
workspace. At that velocity, if the execution of our grasp motion is
\unit[100]{ms} too late, the object will have passed $x_{pred}$ by
more than \unit[3]{cm}, and, depending on the type of the object, it
is very likely that the end-effector will swipe and miss.

\textit{\textbf{Pick Action.}} 
At this stage in the pipeline, it is confirmed that the entire pick action is kinematically feasible. In the case of the pick action, the following two trajectories have been generated:

\begin{itemize}
\item $traj_{p}$ - The pregrasp trajectory begins at the waiting configuration of the arm and ends at $x_{p_{j}}$. It is a collision free path generated by the motion planner, then shortcutted and filtered. $traj_{p}$ has a predicted trajectory execution duration of $d_{p_{pred}}$ seconds.
\item $traj_{g}$ - The grasp trajectory begins with the wrist at $x_{p_{j}}$, then moves to $x_{g_{j}}$ and ends at $x_{po_{j}}$. The path is an open loop motion in which the end-effector moves a very short total distance. $traj_{g}$ is filtered to assure the robot can execute it. Note that we define $d_{g_{pred}}$ as the predicted trajectory execution duration of the trajectory from the start until $x_{g_{j}}$. We disregard the execution time from $x_{g_{j}}$ to $x_{po_{j}}$ because the first half of the motion is the only time sensitive component, given that the end-effector has to be at $x_{g_i}$ to pick up the object at $x_{pred}$ at time $t_{pred}$.
\end{itemize}

The next step is to determine whether there is enough time for the
robot to execute both trajectories in time to pick up the object at
$t_{pred}$. The pickup is determined to be feasible if $t_{pred} >
(t_{now} + d_{p_{pred}} + d_{g_{pred}})$. If the pickup is feasible,
then $traj_{p}$ is executed immediatly. After it is completed,
$traj_{g}$ will begin execution at $t_{g_{execute}} = t_{pred} -
d_{g_{pred}}$. The end-effector is commanded to start closing as it is
approaching the grasp pose. In our experiments, we found that
depending on how early the object was detected and which arm is being
used, the robot would sleep for between 0.0 and 1.5 seconds between
executing the trajectories to ensure a well-timed grasp.


\textit{\textbf{Place Action.}}
After the object is grasped and raised to the postgrasp, there is not much work left to do. The computed path with the upright orientation constraint on the end-effector is executed immediatly. After the end-effector opens, allowing the object to drop into the desired bin, the arm returns to the waiting configuration. The purpose of the waiting configuration is two fold. First, it is intended to keep the arm that is closer to the feed edge of the belt out of the view of the RGB-D sensor, regardless of where the bins are located. Second, it is desirable to keep the arm in a configuration that is close to the table to aid in quicker reaction times by having to execute shorter trajectories.




\section{Experiments}

% We should have a \subsection{PR2 Robot} discussing the PR2. We should try to keep the paper as robot agnostic as possible and here we should have an introduction to our chosen mobile manipulation platform, the pr2.

\subsection{Computing Platforms}
All logic related to planning was executed onboard the PR2's own
computers. The perception software was hosted remotely on a desktop
computer with an Intel Core i7-2600 quad-core CPU at 3.4GHz and an
NVIDIA GeForce GT 420 GPU with 48 CUDA cores, a low-end graphics card
from 2010. The imposition of networking between these two critical
components adds a significant and variable amount of latency to the
system, however the PR2 used for these experiments did not have a
discrete GPU, and so could not host the perception system itself. The
perception pipeline was written in the Haskell functional programming
language, and compiled with the
GHC\footnote{\url{http://www.haskell.org/ghc}} 7.6.1 compiler.

\subsection{Object Selection}
\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/object-group-photo-sm.jpg}
\caption{
The objects used for all experiments.
}
\label{fig:group-photo}
\end{figure}

The objects chosen, Figure~\ref{fig:group-photo}, range in height from
\unit[15]{cm} to \unit[45]{cm}, and breadth from \unit[8]{cm} to
\unit[25]{cm}. These objects also display a variety of rotational
symmetries, which affects their localization and how the objects may
be grasped. The candlesticks and tall green bottle have an infinite
order of rotational symmetry, denoted $C_\infty$, which translates to
a freedom to grasp such an object from any angle, and frees the
localization optimization from needing to consider object
orientation. The shoe is not rotationally symmetric, denoted $C_1$,
and must be grasped from its open end. The spritzer bottle is also
$C_1$, but may be grasped from either side to hook the PR2's fingers
underneath the overhanging geometry of the trigger mechanism. The
various baskets are all $C_2$, as their handles may be grasped from
either side.

Object models were acquired using RoboEarth software
\cite{DiMarco2012:RoboEarthModel}, a poster with fiducial markers, and
a Kinect sensor. The models gathered for the experiments described
here consist of 500 thousand to 1 million points.

\subsection{Pick-and-Place}
Figure~\ref{fig:group-photo} shows the conveyor belt used for testing
both static grasps, in which objects are placed on the surface in
front of the PR2 with the belt motor turned off, and dynamic grasps,
in which the belt motor is on. In the static grasp configuration, the
robot's head is oriented so that it is looking down at the table, and
objects are rapidly placed in front of it. As soon as the robot begins
to clear the work surface with an object in hand, a new object is
placed on the surface. Objects are placed such that at least one arm
can plausibly perform a grasp, but precise position and orientation
are allowed to vary within that constraint.

The experiment conducted in the static grasp configuration involved
100 pick-and-place operations in which the robot removes an object
placed onto the work surface in front of it, and places the object
into one of two bins placed on either side of it. Of the 100 attempted
actions, 91 were successful. The most common failure mode involved the
object slipping out of the robot's gripper due to an insecure
grasp. These 100 actions were timed in blocks of 10, yielding an
average of \unit[6.7]{s} per pick-and-place action. During the time
these experiments were conducted, the perception system failed to
identify an object before a two second timeout period elapsed on two
occasions. The experiments in which perception failed are not included
in the reported time, as we did not have a consistent approach to
failure recovery.

The dynamic configuration shown in the video associated with this
paper has the robot looking down the length of the conveyor belt, as
in Figure~\ref{fig:group-photo}. Objects are placed on the far end of
the \unit[2.13]{m} belt, and carried past the robot. In this
configuration, the perception system only reports on objects it has
seen a minimum of three times. This limits system responsiveness, but
is important to eliminate spurious observations of the object being
hand-placed on the end of the belt, and to ensure stability in pose
estimation.

System performance was measured over 100 pick-and-place operation
attempts with the belt at its top speed, \unitfrac[33]{cm\,}{\,s}, 87
of which were successful. Six objects were effectively ignored due to
the planner being unable to compute a suitable trajectory for either
arm in the allotted time. Of these six, five were spritzer bottles,
suggesting that the grasps chosen for this object did not leave the
planner enough freedom to maneuver. The seven other failures were
fumbled grasps. As in the static test, sometimes an insecure grasp
would lead to an object being dropped. The dynamic test added the new
failure mode of objects bouncing off of the back of the open gripper
during a catch attempt. This contribution of momentum to the
experiment was an excellent test of the system's overall timing: the
gripper had to close around the object as it hooked available geometry
in order to absorb all of its momentum without excessively
destabilizing the object. The tight timing constraints, paired with
the design of the PR2's arms, meant that the robot's arm farther from
where the objects were coming from was easier to utilize. Of the 94
attempted grasps, 57 were made by the far arm.

\section{Analysis}
\subsection{Perception Performance}
Preprocessing includes collecting a point cloud for each object to be
identified. The raw point data is used to populate a \unit[0.5]{cm$^3$}
resolution voxel grid, then passed through a Euclidean Distance
Transform (EDT), taking an average of \unit[304]{ms} for each object.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\columnwidth]{figures/table-density.pdf}
% \caption{
% Table extraction is performed with a recursive histogram over
% candidate connected components of the depth image.
% }
% \label{fig:table-density}
% \end{figure}

Once the system is running, parameters describing the conveyor belt
surface are periodically recalculated as described in
Section~\ref{sec:table-detection}. This process took an average of
\unit[39]{ms} per update, and was run concurrently with the rest of
the perception pipeline in a separate thread.

% Figure~\ref{fig:table-density}
% shows a kernel density estimate of overall table detection
% performance, with a mean processing time of 39ms. This process is run
% concurrently to the rest of the perception pipeline in a separate
% thread, and does not noticeably impact performance of the higher
% frequency components of the perception system.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/refinement.pdf}
\caption{
A nonlinear optimization that fits observed points to a per-object
cost function refines an initial localization estimate provided by
each point cluster's centroid.
}
\label{fig:refinement}
\end{figure}

The localization step begins by considering the centroid of each
cluster of points found above the table surface. This is a reasonably
good estimate of the object's position, but is biased towards the
camera due to self-occlusion of the object geometry. The goal of
navigating the PR2's finger into an opening with approximately
\unit[1]{cm} clearance on either side (e.g. underneath a spritzer
bottle's trigger) demands a level of accuracy in object pose
estimation which can be difficult to achieve when using only point
cluster centroids. However, the localization refinement step
represents a significant fraction of all the time spent in perception,
so its utility is of interest. Recovering object yaw is critical for
grasping asymmetric objects, but focusing solely on translational pose
estimation reveals further contributions made by refining initial
coarse position estimates.

Figure~\ref{fig:refinement} illustrates the translation corrections
made by the optimizer to an initial localization estimate based on
point cluster centroids with a kernel density estimate of observed
refinements. Note that in this run of experiments, there were no false
identifications and no wild misses by the robot. The two object types
needing the greatest correction from their initial centroid estimates
were the big basket and the shoe, which are the two objects with the
greatest asymmetry in the XY plane. The big basket is also a tall
object, resulting in views looking down the table that primarily see
the near side of the object. In contrast, most of the perimeter of an
item like the small basket could be seen at the available view
angle. This contrast is borne out by the effect of the optimization on
these two objects: the big basket's translational position was shifted
an average of \unit[3.4]{cm} from the point cluster centroid, while the small
basket was shifted an average of \unit[0.9]{cm}.

% \begin{table}[t]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Perception Stage & Mean Time (ms)\\ \hline \hline
% Table Extraction & 39.0 \\ \hline\hline
% Cloud Clustering & 22.2 \\ \hline
% Localization & 31.1 \\ \hline
% Total Perception & 53.2 \\ \hline
% \end{tabular}
% \caption{Perception Timing}
% \label{tab:perception-timing}
% \end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/perception-density.pdf}
\caption{ Perception performance broken down by object type. The
  overall mean is \unit[53]{ms}.  }
\label{fig:perception-density}
\end{figure}

The average processing time for the perception system over the 5117
object detections during the moving object experiment was
\unit[53]{ms}, Figure~\ref{fig:perception-density}. The ``\textit{tall
  bottle}'' object was the slowest to process, at an average of
\unit[77]{ms} per update. Initial filtering, clustering, and coarse
recognition comprise the first perception phase, taking an average of
\unit[22.2]{ms}. The second phase consists of the localization
optimization procedure, taking an average of \unit[31.1]{ms}. The
final tracking phase did not take a significant amount of time.

The average processing time for the motion planner during the moving
object experiment was \unit[182]{ms}, including time spent in ROS
service calls. This gives a minimum system response time on the order
of \unit[250]{ms} if only one observation of a scene is needed. For
applications such as dynamic object manipulation, multiple
observations may be needed, which pushes the delay before action to
over \unit[400]{ms}. The performance of these experiments is thus
completely dominated by the speed at which the PR2 can move its
arms. The PR2 requires approximately \unit[4.5]{s} to perform the
motions needed for these tasks, which puts a lower limit on how
quickly the entire system can process pick-and-place tasks.

\subsection{Robustness}
Execution time -- a limiting factor in system responsiveness -- was
consistent throughout the experiments described. The longest single
perception update in the dynamic test covering 5117 updates took
\unit[132]{ms}. There are many sources of variance in these timings as
the perception system was running on the GHC Haskell runtime system
with its generational garbage collector, which itself was running in a
desktop Linux environment (Ubuntu 10.04).

Localization accuracy was measured by considering the PR2's effective
``hand-eye coordination.'' We used the PR2's proprioception to obtain
an estimate of the location of one of the robot's fingertips with its
arm extended over the work surface and angled down so that the finger
just touched the surface. The location of this tangent was marked, and
the arm was removed from the work area. A candidate object was then
placed at the marked location, and the localization returned by the
perception system was compared with the PR2's own estimate of where
its finger had been. 

These experiments yielded an average discrepancy between
proprioception and perception of \unit[5.3]{mm}. A meter stick was
then used to move the object one meter down the work surface from its
starting location. The robot's head was turned to face this new
location, and the output of perception was again recorded. This
relative motion estimate yielded an average discrepancy between the
meter stick and the perception system of \unit[6.7]{mm}.

The last metric recorded from the perception system was the speed of
the belt. This was estimated during the experiment by tracking objects
over time. The perception system estimated the belt speed at
\unitfrac[33.1]{cm\,}{\,s}, with a standard deviation of
\unitfrac[0.08]{cm\,}{\,s}. We were unable to obtain another
measurement of belt speed with less noise.

The reported timings include all segmentation, recognition, and
localization. There is additional overhead in the integrated system
due to running the perception calculations remotely from the PR2
itself. The net result was a ROS object detection publisher rate that
varied between 10 and \unit[12]{Hz}, the same rate at which raw depth
images were received at the perception computer.

\section{Conclusion}
We have demonstrated pick-and-place operations performed by a PR2 at a
rate of \unit[6.7]{s} per object at a 91\% success rate. Similar
operations on a moving work surface yield an 87\% success rate. Room
for improvement remains in the areas of system integration, and
end-effector customization. The speed at which the PR2's arms can move
proved to be a limiting factor in system performance, and the gripper
design was not particularly suited to absorbing the impact of moving
objects. Despite these mechanical limitations, the PR2 proved to be
capable of responsive, high throughput object manipulation.

\section{Acknowledgements}
This research was in part sponsored by the Army Research
Laboratory Cooperative Agreement Number W911NF-10-2-0016.


\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}
