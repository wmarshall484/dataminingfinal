%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out
                                                          % if you need a4paper
%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4
                                                          % paper

\IEEEoverridecommandlockouts                              % This command is only
                                                          % needed if you want to
                                                          % use the \thanks command
\overrideIEEEmargins
% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

\usepackage{graphicx}
\usepackage{subfigure}
\usepackage[]{units}
\usepackage{url}
\usepackage{mathtools}

\title{\LARGE \bf
Analysis of Current Data Mining Techniques on SMS Spam Filtering
%I Got Them Research Paper Writing Blues...
%Intoxicated Cells: a Prospective Study of the Effects of Tequila on Pluripotent Stem Cells
}

\author{William Marshall$^1$%
%\thanks{*This work was supported by SUNFEST}% <-this % stops a space
\thanks{$^2$Lehigh University, USA {\tt\small wcm214@lehigh.edu}}
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}
The necessity of a fast, accurate, easily trainable spam detection
system is reflected in the current proliferation of SMS spam
messages. By taking advantage of the suite of machine learning
algorithms integrated into WEKA, we provide an analysis of SMS spam
detection accuracy across severak different learning models, and
present the Naive Bayes algorithm as a primary choice.
\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}
Short Message Service (SMS) messages are one of the primary means of
communication. Between the years of 2007 and 2010, the number of sent
SMS messages tripled, and the global rate of message sending increased
to 200,000 texts per second [1]. Along with the increased usage of SMS
messages, the proportion of SMS spam messages has also not only risen
to match the new levels of traffic, but has accelerated to the point
where parts of Asia classified 30\% of all SMS messages as spam in
2012 [2]. Although SMS spam is not as prevelant as email spam -- which
constituted 70.7\% of all emails in Q2 2013 -- there is still a strong
motivation for cellular companies to employ effective spam detection
systems, since it's likely that customers are willing to pay more for
a service that is insulated from spam.

Many modern approaches to spam detection are founded upon the research
that stems from the identification of email spam. Email spam detection
benefits from features such as a subject header, initial greetings,
and the IP address of the sender. Such features are not applicable to
SMS messages which have an upper limit of 160 characters, providing
overall less information to perform classification. Additionally,
since SMS spam is less pervasive than email spam, its classification
is made more difficult due to the lower prior probability. Blanzieri
at al. were able to achieve an email spam misclassification rate of
only 1.17\% [3], which is a daunting task for SMS spam detection.

The differences between SMS and email spam detection warrant a
reevaluation of the effectiveness of current learning
models. Fortunately, the University of Waikato has provided WEKA: a
toolkit that conveniently provides implementations of many well known
and vetted classification, association, and clustering techniques. The
algorithms included in WEKA and tested in this paper are by no means
an exhaustive analysis of the spam detetion literature, yet this paper
should function well as an introduction for those who are new to
machine learning, WEKA, and spam detection.

\section{Dataset}
The dataset in question was collected from the UCI Machine learning
repository,which contains 5574 text messages, 425 ofwhich were spam,
the rest of which are non-spam. This dataset was chosen because its
spam to non-spam ratio of 7.6\% roughly approximates the SMS spam rate
found in nature, since the spam rate in the UK is around 20\% [6],
while the SMS spam rate inthe US is less than 1\%. The data was
collected from a website called grumbletext.com,where users post the
text of spam messages that they have recieved, and was manually
inspected and classified.

It should be noted that there is an inherent bias in the dataset,
since the messages were collected from cellular users in the United
Kingdom. As a result of this, certain features that are a strong
indicator of spam (such as the presence of the ``Â£'' character) are
not as applicable to messages recieved in the United States. Such
geographic, culture, and language-based biases are unavoidable, and
actual implementations of the following learning models would do well
to train on messages from the intended location.

\section{Preprocessing}
\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{figures/spam_ham.png}
\caption{ The SMS spam detectin system is trained on a dataset
  containing 5,574 text messages, 425 of which are spam and the rest
  non-spam. The messages were taken from a UK website where users post
  spam messages. The messages were inspected and classified manually.
}
\label{fig:perception-pipeline}
\end{figure}

\subsection{Token and Frequency Extraction}

The data was provided as raw text, where each line corresponded to a
class label and a space, followed by the text of the SMS message. The
end goal of the preprocessing stage is to transform the data into a
.arff format, where each unique word corresponds to a nominal feature
that is either a 1, if present in the text message, or a 0 if it was
not present. As such, it was necessary to take to union of each SMS
message's words across all messages in the dataset. This was
accomplished using the std::map class included in the standard
template library of c++.  The initial preprocessing, resulted in
18,152 unique words.  

Upon examination, it was discovered that many of the words contained
special characters that falsely inflated the number of unique words;
for example, in addition to having an entry for the string ``apple'',
there would also be an entry for the string: ``apple.'', ``apple!'',
and ``apple?''. Thus, it was necessary to remove all special
characters that might cause confusion within a unique feature, so any
character within the following
string \begin{verbatim}!@#\%^&*()<>.,\end{verbatim} was replaced with
a space (subequently, all spurious spaces are removed). After this
preprocessing stage, only 12,734 unique strings remained.

Many of the words that remained were ones such as ``the'' or ``a''
that were ubiquitous across both spam and non-spam messages. Rather
than have high frequency yet low information features, the strings
that were used more that 500 or less than 5 times were simply deleted
from the feature space. In addition, the top 50 most frequent words
were alse deleted, finally resulting in 8,185 unique strings. 

\subsection{Chi-squared selection}

Unfortunately, having several thousands of attributes is prohibitively
expensive for learning techniques that don't scale well with
dimensionality. Our attempts to run the J48 decision-tree learning on
insatances 8,185 attributes took several days, and eventually resulted
in an overflow of the JVM's heap space. Therefore, it was necessary to
further reduce the dimensionality to even begin to obtain data.

The chi-squared independence test is an indicator for how well
observed data corresponds to a statistical model. The idiomatic
example is to determine, at a certain level of confidence, whether a
coin toss is baised based on 100 flips, where 66 tosses are heads and
33 are tails. Chi-squared is calculated as follows:

\[\sum_{i=1}^{N}=\frac{(e_i-a_i)^2}{e_i}\]

Where $e_i$ represents an expected value, $a_i$ represents the
observed (actual) value, and $N$ is the total number of expected
values. In the case of the coin example, there are only two expected
values: the number of heads and the number of tails, each of which has
an expected value of 50 if the coin is fair. Therefore, the
chi-squared value for 66 heads and 33 tails is 10.9, which is greater
than the value of 7.88 for a 95\% confidence interval, indicating that
the coin is biased.

This directly applies to the problem of feature selection, where the
primary goal is to find features that are not indepenent to the set of
classes; e.g., the selection of features whose presence or absence is
an indicator for its class. To this end, we calculate the chi-squared
metric for each of the 8,185 remaining features, and retained the
ones that demonstrate class dependence, resulting in 1,835 attributes.

\section{Classification}

The remaining attributes were few enough to allow the employed
learning methods to complete in a reasonable amount of time using
10-fold cross validation. Although several of the following methods
take large amounts of time to build despite the relatively small
dataset, this results partially from WEKA being largely
single-threaded. Actual implementations of these methods for industry
usage would benefit greatly from exploiting the parallel structure of
techniques such as Naive Bayes or J48.

\subsection{EM Clustering}

The Expectation-Maximization algorithm (EM) proved to be the slowest,
yet most efficient algorithm. The EM clustering techniques operates by
fitting hyperellipsoids to the dataset.  It iterates over two stages
-- the expectation and maximization steps -- and modifies the
parameters of the hyperellipsoids until it converges to a
solution. During the expectation step, the algorithm creates an
indicator for how well the dataset complies with the model. In the
maximization step, it determines which slight modifications to the
model parameters would maximize the fit, and applies them.

The preprocessing required to train EM was minimal. In order to ensure
that the EM is agnostic to an instance's class during the clustering,
the class labels were stripped from the instances prior to training,
but no further modifications were required.

EM clustering had an accuracy of 99.31\%, with only 42 instances
misclassified. Although its ultimate accuracy is impressive, the time
it took to build the model was by far the largest of any of the tested
methods, taking over 2 hours to complete on a modern intel
processor. EM clustering has a computational complexity of $O(n*k*d)$,
where n is the number of instances, k is the number of clusters, and d
is the number of attributes. Since EM scales linearly with the number
of dimensions, it might seem strange that it had such a high
runtime. This might result from two facters: firstly, EM might have a
large constant, despite the linear scaling. Secondly, it is possible
that this results from having too low a value of the final
log-likelihood parameter which determines when the clustering algoritm
terminates. Unfortunately, it was beyond the time constraints of this
project to determine whether the default log-likelihood value was
optimal.

\begin{figure}[t]
\centering
\includegraphics[width=0.93\columnwidth]{figures/EM.png}
\caption{ The result of the EM clustering and classification from left
  to right: the actual class of the instance, the number of instances
  classified as spam, and the number of instances classified as
  non-spam.  }
\label{fig-3}
\end{figure}


\subsection{K Nearest Neighbor}
EM's high accuracy, yet unfeasable runtime demonstrated the need for a
classification method that run faster, even at the expense of
accuracy. The K Nearest Neighbor method (KNN) provides such a
compromise; it is one of the most widely used machine learning methods
due to its reasonably high dimensional-scalability and accuracy. One
of the most appealing aspects of KNN is that it has been proved that
its accuracy converges to no worse than twice the Bayes error rate if
given a sufficient number of instances. This is seen in the following
equation:

\[
P^* \leq P \leq P^*(2-\frac{c}{c-1}P^*) 
\]

where $P^*$ is the Bayes error, $P$ is the error associated with KNN,
and $c$ is the number of classes.

For each instance, KNN operates by iterating over each instance in the
training set, and keeping track of the N closest instances and their
classes. The test instance is classified according to the class that
has the highest proportion among the N closest points. Like EM
clustering, the runtime of KNN scales linearly with the dimensionality
of the dataset, since its complexity is $O(n*m*d)$. Moreover, KNN
classification requires no preprocessing of the data (in the absence
of a kd-tree) thus taking no time to build a KNN model. While not
requiring time to build a model might seem like a benefit because it
saves time, it is far less space efficient than other methods; in
order to classify future instances, EM clustering only need to store a
few parameters for its hyperellipsoids while KNN must retain the
entire set of training instances. For SMS spam detection software that
might need to run on a cellular phone, a gigabyte-sized training set
might prove too large.

For values of K that ranged from 1-5, K nearest neighbor had an
accuracy of 92.05\%, taking an average time of 91 seconds. Compared to
the runtime of EM clustering, KNN presents itself as a viable
alternative. An interesting trend occurs when oberving the accuracies
given different values of K; as the value of K increases, the accuracy
of KNN classification decreases. Usually, higher values of K
corresponds with higher accuracy values because it causes KNN to be
more tightly bound to the Bayes error. A possible explanation is that
there is insufficient data to completely map the feature space,
resulting in a bias towards the class that has the larger prior
probability. Given more data, it's likely that larger values of K
would instead positively affect KNN's accuracy.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/KNN.png}
\caption{
The accuracy of the KNN algorithm on the SMS dataset for varying values of K. A distinct downwards trend is noticed as the value of K increases.
}
\label{fig:fig4}
\end{figure}


\subsection{J48}

J48 decision trees, another staple of machine learning techniques,
were also tested. The J48 algorithm is a modification of the ID3
agorithm that operates on the principle of iteratively removing
informational entropy from the dataset by splitting instances into
groups based on the values of a nominal attribute at each
iteration. Informational entropy is an indicator of the degree to
which a dataset is divided amongst its classes, and can be calculated
using the following formula:

\[
\sum_{i=1}^{C} p_i\log{p_i}
\]

where $C$ represents the number of classes, and $p$ represents the the
proportion of class $i$ in the dataset. For example: a dataset where
90\% of its instances a class $A$, and only 10\% of its instances are
in a class $B$ would have lower informational entropy than a dataset
that has 50\% of its instances in $A$ and $B$, respectively.

J48 begins by calculating the initial entropy of the dataset, the
average entropy of the resulting datasets after splitting on an
attribute, and the difference between the two termed the ``information
gain''. Whichever attribute yields the largest information gain is
chosen to be the root of the decision tree. The process is then
repeated recursively for each subdivided dataset until a complete tree
is created.

Because J48 recirsively tests each subdivided dataset on each
remaining attribute, its computational complexity is no less than
$O(d^2)$. Despite the squared dimensional complexity of J48, its
runtime on the SMS dataset was less than the EM clustering algorithm,
terminating in about half an hour, however its accuracy was also lower
than the EM clustering's with 96.9\% accuracy.

An advantage to using J48 is that the decision tree that it generates
is mostly human-readable. For the idiomatic applications such as
doctors deciding whether to give a patient contact lenses, this is
highly beneficial. For the purposes of SMS spam detection, we are
given insight into the strings and tokens which are the best
indicators of whether a message is or isn't spam. According to the
output of J48, the string that removes the most entropy from the spam
dataset is the string ``claim'', followed by ``www'' and
``prize''. Intuitively it should make sense, since many spam messages
have the format ``Claim your prize at www.spamsite.com!'', yet it is
reassuring to have this appear in our findings. Unfortunately, the J48
model that resulted from the SMS dataset was rather large, having a
size of 139 with 70 leaf-nodes, and is arguably too complex to be
human readable. Even so, while such a large tree might be
inappropriate for human-readibility, the model is orders of magnitude
smaller than the KNN model.

\subsection{Naive Bayes}
Of all the learning models that were tested, Naive Bayes proved to be
the most effective method for SMS spam detection. Naive Operates on
Bayes' theorem of conditional probablility which is expressed in its
asymmetric form as follows:

\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\]

In other words, given two events $A$ and $B$, the probability that
both $A$ and $B$ occur simultaneously is equal to the prior
probability of one times the conditional probability of the
other. Bayes' theorem applies directly to the problem of SMS spam
detection because we can use it to calculate $P(spam| \mathbf{x})$;
e.g., the probability that a particular message is spam given the
words that were used in the feature vector $\mathbf{x}$.

Using WEKA's Naive Bayes classification method resulted in a
classification accuracy of 98.277\%, the most accurate of all the
faster-running algorithms. Unlike the previous techniques explored in
this paper, the accuracy Naive Bayes did not come at the cost of slow
runtime, performing its classifications in only 0.82 seconds per fold
of cross-validation. Additionally, the model that it uses for
classification consists only of the prior and posterior probabilities
of each class and attribute value, respectively, since the
classification computation is only a simple multiplication of these
values. As such, Naive Bayes scales linearly with the dimension of the
dataset, implying decent runtime performance even when the number of
unique words increases by orders of magnitude.

It's likely that Naive Bayes' high accuracy rate stems partially from
the chi-squared selection generated features that were optimal. Naive
Bayes performs well when the class features are not independent of
their class; that is, when the conditional probabilities of each class
attribute deviate from a value of $\frac{1}{c}$. The chi-squared metric
selects such features.

\section{Conclusion and future work}

Although Naive Bayes performed better than any of the other methods tested, it still
 

\begin{figure}[t]
\centering
\includegraphics[width=0.8\columnwidth]{figures/J48.png}
\caption{
The first four decisions in the J48 decision tree.
}
\end{figure}
\subsection{Object Selection}
The objects chosen, Figure~\ref{fig:group-photo}, range in height from
\unit[15]{cm} to \unit[45]{cm}, and breadth from \unit[8]{cm} to
\unit[25]{cm}. These objects also display a variety of rotational
symmetries, which affects their localization and how the objects may
be grasped. The candlesticks and tall green bottle have an infinite
order of rotational symmetry, denoted $C_\infty$, which translates to
a freedom to grasp such an object from any angle, and frees the
localization optimization from needing to consider object
orientation. The shoe is not rotationally symmetric, denoted $C_1$,
and must be grasped from its open end. The spritzer bottle is also
$C_1$, but may be grasped from either side to hook the PR2's fingers
underneath the overhanging geometry of the trigger mechanism. The
various baskets are all $C_2$, as their handles may be grasped from
either side.

Object models were acquired using RoboEarth software
\cite{DiMarco2012:RoboEarthModel}, a poster with fiducial markers, and
a Kinect sensor. The models gathered for the experiments described
here consist of 500 thousand to 1 million points.

\subsection{Pick-and-Place}
Figure~\ref{fig:group-photo} shows the conveyor belt used for testing
both static grasps, in which objects are placed on the surface in
front of the PR2 with the belt motor turned off, and dynamic grasps,
in which the belt motor is on. In the static grasp configuration, the
robot's head is oriented so that it is looking down at the table, and
objects are rapidly placed in front of it. As soon as the robot begins
to clear the work surface with an object in hand, a new object is
placed on the surface. Objects are placed such that at least one arm
can plausibly perform a grasp, but precise position and orientation
are allowed to vary within that constraint.

The experiment conducted in the static grasp configuration involved
100 pick-and-place operations in which the robot removes an object
placed onto the work surface in front of it, and places the object
into one of two bins placed on either side of it. Of the 100 attempted
actions, 91 were successful. The most common failure mode involved the
object slipping out of the robot's gripper due to an insecure
grasp. These 100 actions were timed in blocks of 10, yielding an
average of \unit[6.7]{s} per pick-and-place action. During the time
these experiments were conducted, the perception system failed to
identify an object before a two second timeout period elapsed on two
occasions. The experiments in which perception failed are not included
in the reported time, as we did not have a consistent approach to
failure recovery.

The dynamic configuration shown in the video associated with this
paper has the robot looking down the length of the conveyor belt, as
in Figure~\ref{fig:group-photo}. Objects are placed on the far end of
the \unit[2.13]{m} belt, and carried past the robot. In this
configuration, the perception system only reports on objects it has
seen a minimum of three times. This limits system responsiveness, but
is important to eliminate spurious observations of the object being
hand-placed on the end of the belt, and to ensure stability in pose
estimation.

System performance was measured over 100 pick-and-place operation
attempts with the belt at its top speed, \unitfrac[33]{cm\,}{\,s}, 87
of which were successful. Six objects were effectively ignored due to
the planner being unable to compute a suitable trajectory for either
arm in the allotted time. Of these six, five were spritzer bottles,
suggesting that the grasps chosen for this object did not leave the
planner enough freedom to maneuver. The seven other failures were
fumbled grasps. As in the static test, sometimes an insecure grasp
would lead to an object being dropped. The dynamic test added the new
failure mode of objects bouncing off of the back of the open gripper
during a catch attempt. This contribution of momentum to the
experiment was an excellent test of the system's overall timing: the
gripper had to close around the object as it hooked available geometry
in order to absorb all of its momentum without excessively
destabilizing the object. The tight timing constraints, paired with
the design of the PR2's arms, meant that the robot's arm farther from
where the objects were coming from was easier to utilize. Of the 94
attempted grasps, 57 were made by the far arm.

\section{Analysis}
\subsection{Perception Performance}
Preprocessing includes collecting a point cloud for each object to be
identified. The raw point data is used to populate a \unit[0.5]{cm$^3$}
resolution voxel grid, then passed through a Euclidean Distance
Transform (EDT), taking an average of \unit[304]{ms} for each object.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.8\columnwidth]{figures/table-density.pdf}
% \caption{
% Table extraction is performed with a recursive histogram over
% candidate connected components of the depth image.
% }
% \label{fig:table-density}
% \end{figure}

Once the system is running, parameters describing the conveyor belt
surface are periodically recalculated as described in
Section~\ref{sec:table-detection}. This process took an average of
\unit[39]{ms} per update, and was run concurrently with the rest of
the perception pipeline in a separate thread.

% Figure~\ref{fig:table-density}
% shows a kernel density estimate of overall table detection
% performance, with a mean processing time of 39ms. This process is run
% concurrently to the rest of the perception pipeline in a separate
% thread, and does not noticeably impact performance of the higher
% frequency components of the perception system.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/refinement.pdf}
\caption{
A nonlinear optimization that fits observed points to a per-object
cost function refines an initial localization estimate provided by
each point cluster's centroid.
}
\label{fig:refinement}
\end{figure}

The localization step begins by considering the centroid of each
cluster of points found above the table surface. This is a reasonably
good estimate of the object's position, but is biased towards the
camera due to self-occlusion of the object geometry. The goal of
navigating the PR2's finger into an opening with approximately
\unit[1]{cm} clearance on either side (e.g. underneath a spritzer
bottle's trigger) demands a level of accuracy in object pose
estimation which can be difficult to achieve when using only point
cluster centroids. However, the localization refinement step
represents a significant fraction of all the time spent in perception,
so its utility is of interest. Recovering object yaw is critical for
grasping asymmetric objects, but focusing solely on translational pose
estimation reveals further contributions made by refining initial
coarse position estimates.

Figure~\ref{fig:refinement} illustrates the translation corrections
made by the optimizer to an initial localization estimate based on
point cluster centroids with a kernel density estimate of observed
refinements. Note that in this run of experiments, there were no false
identifications and no wild misses by the robot. The two object types
needing the greatest correction from their initial centroid estimates
were the big basket and the shoe, which are the two objects with the
greatest asymmetry in the XY plane. The big basket is also a tall
object, resulting in views looking down the table that primarily see
the near side of the object. In contrast, most of the perimeter of an
item like the small basket could be seen at the available view
angle. This contrast is borne out by the effect of the optimization on
these two objects: the big basket's translational position was shifted
an average of \unit[3.4]{cm} from the point cluster centroid, while the small
basket was shifted an average of \unit[0.9]{cm}.

% \begin{table}[t]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% Perception Stage & Mean Time (ms)\\ \hline \hline
% Table Extraction & 39.0 \\ \hline\hline
% Cloud Clustering & 22.2 \\ \hline
% Localization & 31.1 \\ \hline
% Total Perception & 53.2 \\ \hline
% \end{tabular}
% \caption{Perception Timing}
% \label{tab:perception-timing}
% \end{table}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/perception-density.pdf}
\caption{ Perception performance broken down by object type. The
  overall mean is \unit[53]{ms}.  }
\label{fig:perception-density}
\end{figure}

The average processing time for the perception system over the 5117
object detections during the moving object experiment was
\unit[53]{ms}, Figure~\ref{fig:perception-density}. The ``\textit{tall
  bottle}'' object was the slowest to process, at an average of
\unit[77]{ms} per update. Initial filtering, clustering, and coarse
recognition comprise the first perception phase, taking an average of
\unit[22.2]{ms}. The second phase consists of the localization
optimization procedure, taking an average of \unit[31.1]{ms}. The
final tracking phase did not take a significant amount of time.

The average processing time for the motion planner during the moving
object experiment was \unit[182]{ms}, including time spent in ROS
service calls. This gives a minimum system response time on the order
of \unit[250]{ms} if only one observation of a scene is needed. For
applications such as dynamic object manipulation, multiple
observations may be needed, which pushes the delay before action to
over \unit[400]{ms}. The performance of these experiments is thus
completely dominated by the speed at which the PR2 can move its
arms. The PR2 requires approximately \unit[4.5]{s} to perform the
motions needed for these tasks, which puts a lower limit on how
quickly the entire system can process pick-and-place tasks.

\subsection{Robustness}
Execution time -- a limiting factor in system responsiveness -- was
consistent throughout the experiments described. The longest single
perception update in the dynamic test covering 5117 updates took
\unit[132]{ms}. There are many sources of variance in these timings as
the perception system was running on the GHC Haskell runtime system
with its generational garbage collector, which itself was running in a
desktop Linux environment (Ubuntu 10.04).

Localization accuracy was measured by considering the PR2's effective
``hand-eye coordination.'' We used the PR2's proprioception to obtain
an estimate of the location of one of the robot's fingertips with its
arm extended over the work surface and angled down so that the finger
just touched the surface. The location of this tangent was marked, and
the arm was removed from the work area. A candidate object was then
placed at the marked location, and the localization returned by the
perception system was compared with the PR2's own estimate of where
its finger had been. 

These experiments yielded an average discrepancy between
proprioception and perception of \unit[5.3]{mm}. A meter stick was
then used to move the object one meter down the work surface from its
starting location. The robot's head was turned to face this new
location, and the output of perception was again recorded. This
relative motion estimate yielded an average discrepancy between the
meter stick and the perception system of \unit[6.7]{mm}.

The last metric recorded from the perception system was the speed of
the belt. This was estimated during the experiment by tracking objects
over time. The perception system estimated the belt speed at
\unitfrac[33.1]{cm\,}{\,s}, with a standard deviation of
\unitfrac[0.08]{cm\,}{\,s}. We were unable to obtain another
measurement of belt speed with less noise.

The reported timings include all segmentation, recognition, and
localization. There is additional overhead in the integrated system
due to running the perception calculations remotely from the PR2
itself. The net result was a ROS object detection publisher rate that
varied between 10 and \unit[12]{Hz}, the same rate at which raw depth
images were received at the perception computer.

\section{Conclusion}
We have demonstrated pick-and-place operations performed by a PR2 at a
rate of \unit[6.7]{s} per object at a 91\% success rate. Similar
operations on a moving work surface yield an 87\% success rate. Room
for improvement remains in the areas of system integration, and
end-effector customization. The speed at which the PR2's arms can move
proved to be a limiting factor in system performance, and the gripper
design was not particularly suited to absorbing the impact of moving
objects. Despite these mechanical limitations, the PR2 proved to be
capable of responsive, high throughput object manipulation.

\section{Acknowledgements}
This research was in part sponsored by the Army Research
Laboratory Cooperative Agreement Number W911NF-10-2-0016.


\bibliographystyle{IEEEtran}
\bibliography{refs}
\end{document}
